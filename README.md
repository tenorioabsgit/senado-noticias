# ğŸ“° Coletor de NotÃ­cias do Senado Federal

Este projeto realiza **web scraping** de notÃ­cias publicadas no portal oficial do **Senado Federal**, permitindo a extraÃ§Ã£o de **tÃ­tulos, links, datas e conteÃºdos completos** das matÃ©rias.

## ğŸ“Œ Funcionalidades

âœ… Busca de notÃ­cias relacionadas a um termo especÃ­fico  
âœ… ExtraÃ§Ã£o automÃ¡tica de **tÃ­tulo, conteÃºdo, link e data da publicaÃ§Ã£o**  
âœ… Coleta de mÃºltiplas pÃ¡ginas para obter resultados extensivos  
âœ… OrganizaÃ§Ã£o dos dados em um **arquivo CSV**  

---

## ğŸ›  Tecnologias Utilizadas

- **Linguagem:** R  
- **Pacotes:** `rvest`, `tidyverse`, `stringr`, `lubridate`  
- **Fonte de dados:** [Senado Federal](https://www12.senado.leg.br/noticias/)  

---

## ğŸ“‚ Estrutura do CÃ³digo

```
ğŸ“ /  (DiretÃ³rio Raiz)
â”œâ”€â”€ noticias_senado.R       # Script principal para coleta de notÃ­cias
â””â”€â”€ noticias_senado.csv     # Arquivo de saÃ­da com as notÃ­cias extraÃ­das
```

---

## ğŸš€ Como Executar

### 1ï¸âƒ£ **Instalar os pacotes necessÃ¡rios**

Se ainda nÃ£o estiverem instalados, execute no R:

```r
install.packages(c("rvest", "tidyverse", "stringr", "lubridate"))
```

### 2ï¸âƒ£ **Modificar o termo de pesquisa**

No script `noticias_senado.R`, edite a variÃ¡vel `termo_pesquisa` para buscar por um nome ou tema diferente:

```r
termo_pesquisa <- "InteligÃªncia Artificial"
```

### 3ï¸âƒ£ **Executar o Script**

Para rodar o script e coletar as notÃ­cias:

```r
source("noticias_senado.R")
```

Os resultados serÃ£o armazenados em um **dataframe** e podem ser salvos em um arquivo CSV.

---

## ğŸ“Š Estrutura do Arquivo de SaÃ­da (`noticias_senado.csv`)

A saÃ­da serÃ¡ um CSV com as seguintes colunas:

| Data | TÃ­tulo | ConteÃºdo | Link |
|------|--------|----------|------|
| 2024-02-12 | TÃ­tulo da NotÃ­cia | Texto completo da matÃ©ria | URL da notÃ­cia |

---

## ğŸ›‘ PossÃ­veis Problemas e SoluÃ§Ãµes

### âš ï¸ **Erro ao acessar o site do Senado**
Se a conexÃ£o for bloqueada, experimente rodar o script em um horÃ¡rio diferente ou verificar sua conexÃ£o de internet.

### â³ **A coleta parece lenta?**
Caso queira **reduzir a quantidade de notÃ­cias coletadas**, diminua o intervalo de busca:

```r
intervalo_urls <- seq(from = 0L, to = 500, by = 30)  # Busca apenas as primeiras 500 notÃ­cias
```

### ğŸ”„ **Problemas com formataÃ§Ã£o da data**
Se houver erro ao converter datas, tente modificar a lÃ³gica de extraÃ§Ã£o da data dentro do script:

```r
data <- lubridate::dmy(data_extraida)
```

---

ğŸš€ **Agora vocÃª pode coletar notÃ­cias automaticamente!** Qualquer dÃºvida, me avise! ğŸ˜Š
